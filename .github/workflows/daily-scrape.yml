name: Daily Job Scraper (Automated)

on:
  schedule:
    - cron: "0 6,18 * * *"
  workflow_dispatch:

concurrency:
  group: daily-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    # NOTE:
    # Scheduled workflows always run from the repo DEFAULT branch.
    # This allows the job to run even if GitHub shows the run under "main".
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/develop'

    steps:
      - name: Trigger Railway Scraper API
        shell: bash
        run: |
          set -euo pipefail

          echo "üöÄ Triggering job scraper on Railway..."
          echo "Event: $GITHUB_EVENT_NAME"
          echo "Ref:   $GITHUB_REF"

          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "https://www.6figjobs.com/api/cron/scrape" \
            -H "Authorization: Bearer ${{ secrets.CRON_SECRET }}" \
            -H "Content-Type: application/json" \
            --max-time 600 \
            --retry 2 \
            --retry-delay 30)

          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | head -n-1)

          echo "üìä Response Code: $HTTP_CODE"
          echo "üìÑ Response Body: $BODY"

          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo "‚úÖ Scraper triggered successfully"
            exit 0
          else
            echo "‚ùå Scraper failed with code $HTTP_CODE"
            exit 1
          fi

      - name: Report Failure
        if: failure()
        run: |
          echo "‚ö†Ô∏è Scraper failed!"
          echo "üîç Check Railway logs in your Railway project Deployments tab."
